{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71bcc6-e31c-46dc-9dac-1d22e5b142d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates the training of the DQN model described in Section 3.\n",
    "# It aligns with the environment design, reward shaping, and architecture used in the published study.\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the RL Environment for Construction Logistics\n",
    "class ConstructionLogisticsEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ConstructionLogisticsEnv, self).__init__()\n",
    "\n",
    "        self.truck_capacity = 10                   # Each truck can deliver 10 tons\n",
    "        self.valid_order_quantities = [0, 10, 20, 30, 40]  # Only full truckloads allowed\n",
    "        self.max_inventory = 60                    # Site can store 60 tons max\n",
    "        self.max_order = max(self.valid_order_quantities)  # Maximum possible per day\n",
    "        \n",
    "        self.storage_cost = 2                     # £2/ton/day (premium urban cost)\n",
    "        self.shortage_cost = 100                  # £100/ton for delay-related downtime\n",
    "        self.transport_cost = 100                 # £100/truck (fixed per truck, not per ton)\n",
    "        \n",
    "        self.time_horizon = 11\n",
    "        self.current_time = 0\n",
    "        self.inventory_level = 0\n",
    "        self.lead_time = []\n",
    "        self.pending_orders = deque()\n",
    "        self.backlog = 0\n",
    "        self.recent_truck_usage = [0, 0]  # usage on t-1 and t-2\n",
    "        \n",
    "        # Observation space adjusted for batching logic and max capacity\n",
    "        # State = [inventory, day, backlog, expected_demand_today, lead_time, expected_demand_tomorrow, trucks used on day t-1 and t-2]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 1, 0, 0, 0], dtype=np.float32),\n",
    "            high=np.array([\n",
    "                self.max_inventory,       # inventory\n",
    "                self.time_horizon,        # current time\n",
    "                2 * self.max_inventory,   # backlog\n",
    "                20,                       # expected demand\n",
    "                2,                        # lead time\n",
    "                20,                       # next day demand\n",
    "                4,                        # trucks used on day t-1\n",
    "                4                        # trucks used on day t-2\n",
    "            ], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Updated action space to reflect only batch sizes\n",
    "        self.action_space = gym.spaces.Discrete(len(self.valid_order_quantities))\n",
    "      \n",
    "\n",
    "    def step(self, action):\n",
    "        # Receive arriving orders\n",
    "        while self.pending_orders and self.pending_orders[0][1] == self.current_time:\n",
    "            delivery_qty = self.pending_orders.popleft()[0]\n",
    "            available_space = self.max_inventory - self.inventory_level\n",
    "            actual_delivery = min(delivery_qty, available_space)\n",
    "            self.inventory_level += actual_delivery#\n",
    "            \n",
    "        # Generate random demand influences\n",
    "        self.weather_factor = np.random.choice([0.6, 0.85, 1.0], size=self.time_horizon, p=[0.02, 0.1, 0.88])\n",
    "        self.crane_breakdown = np.random.choice([0.5, 1.0], size=self.time_horizon, p=[0.02, 0.98])\n",
    "        self.worker_productivity = np.random.choice([0.8, 1.0], size=self.time_horizon, p=[0.05, 0.95])\n",
    "        self.trnasport_latness = np.random.choice([0.8, 0.9, 1.0], size = self.time_horizon,p= [0.1,0.8, 0.1])\n",
    "        \n",
    "        # Generate stochastic demand     \n",
    "        raw_demand = self.expected_demand[self.current_time]       \n",
    "        adjusted_demand = raw_demand * self.weather_factor[self.current_time] \\\n",
    "                                        * self.crane_breakdown[self.current_time] \\\n",
    "                                        * self.worker_productivity[self.current_time]\\\n",
    "                                        * self.trnasport_latness[self.current_time]\n",
    "        if self.current_time == 0:\n",
    "            demand = 0\n",
    "        else:\n",
    "            demand = int(adjusted_demand) + self.backlog\n",
    "\n",
    "        \n",
    "        # Fulfill demand       \n",
    "        shortage = max(0, demand - self.inventory_level)\n",
    "\n",
    "        fulfilled = demand - shortage\n",
    "\n",
    "        self.inventory_level -= fulfilled\n",
    "\n",
    "        self.backlog = shortage  # carry unmet demand to tomorrow\n",
    "\n",
    "        \n",
    "        # Place order if valid\n",
    "        order_quantity = self.valid_order_quantities[action]\n",
    "        num_trucks_today = order_quantity // self.truck_capacity\n",
    "        self.recent_truck_usage.append(num_trucks_today)\n",
    "        self.recent_truck_usage = self.recent_truck_usage[-2:]\n",
    "        if order_quantity > 0:\n",
    "            delivery_time = self.current_time + self.lead_time[self.current_time]\n",
    "            if delivery_time < self.time_horizon:\n",
    "                self.pending_orders.append((order_quantity, delivery_time))\n",
    "\n",
    "        # --- Base reward (negative cost) ---    \n",
    "        holding_cost = self.storage_cost * self.inventory_level\n",
    "\n",
    "        shortage_penalty = self.shortage_cost * shortage\n",
    "\n",
    "        transport_cost = num_trucks_today * self.transport_cost\n",
    "        \n",
    "        total_cost =  transport_cost + holding_cost + shortage_penalty \n",
    "        reward = -total_cost  # Default baseline reward\n",
    "\n",
    "            \n",
    "         # --- Improved Reward Shaping (Just-in-Time Incentive) ---\n",
    "                \n",
    "        # 1. Bonus for exact match (ideal inventory = tomorrow's demand)\n",
    "        if 0 < self.current_time < self.time_horizon - 1:\n",
    "            ideal_inventory = self.expected_demand[self.current_time + 1]  # anticipate tomorrow\n",
    "            diff = abs(self.inventory_level - ideal_inventory)\n",
    "            reward += max(0, 20 - 2 * diff)  # bonus decreases as deviation grows\n",
    "        \n",
    "        # 2. Penalty for excessive stock (beyond what's needed for next 2 days)\n",
    "        if self.current_time < self.time_horizon - 2:\n",
    "            next_two_days_demand = self.expected_demand[self.current_time + 1] + self.expected_demand[self.current_time + 2]\n",
    "            if self.inventory_level > next_two_days_demand + 5:\n",
    "                penalty = (self.inventory_level - next_two_days_demand) * 1.5\n",
    "                reward -= penalty\n",
    "        \n",
    "        # 3. End-of-horizon inventory penalty (already included, keep this)\n",
    "        if self.current_time == self.time_horizon - 1:\n",
    "            excess_penalty = self.inventory_level * 5\n",
    "            reward -= excess_penalty\n",
    "        \n",
    "        # 4. JIT Bonus for low inventory AND no shortage\n",
    "        if shortage == 0 and self.inventory_level <= 2:\n",
    "            reward += 30\n",
    "        \n",
    "        # 5. Deterrent for ordering when inventory is already near max\n",
    "        if action > 0 and self.inventory_level > self.max_inventory * 0.8:\n",
    "            reward -= 10\n",
    "            \n",
    "        next_day_demand = self.expected_demand[self.current_time + 1] if self.current_time + 1 < self.time_horizon else 0\n",
    "        \n",
    "        self.current_time += 1\n",
    "        done = self.current_time >= self.time_horizon\n",
    "    \n",
    "        if done:     \n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32), reward, done, {\"cost\": total_cost}\n",
    "    \n",
    "        state = np.array([\n",
    "            self.inventory_level,                      # Current inventory\n",
    "            self.current_time,                         # Current day\n",
    "            self.backlog,                              # Unfulfilled demand\n",
    "            self.expected_demand[self.current_time],   # Based on planned site activities\n",
    "            self.lead_time[self.current_time],          # Current lead time\n",
    "            next_day_demand,\n",
    "            self.recent_truck_usage[0],\n",
    "            self.recent_truck_usage[1]\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "        return state, reward, done, {\"cost\": total_cost}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_time = 0\n",
    "        self.inventory_level = 0\n",
    "        self.backlog = 0        \n",
    "        self.lead_time = np.random.choice([1, 2], size=self.time_horizon, p=[0.9, 0.1])\n",
    "        self.pending_orders = deque()\n",
    "        #self.expected_demand = np.full(self.time_horizon, 10)\n",
    "        self.expected_demand = np.array([12, 14, 10, 8, 18, 20, 10, 10, 14, 10, 12])\n",
    "        next_day_demand = self.expected_demand[1] if self.time_horizon > 1 else 0\n",
    "        self.recent_truck_usage = [0, 0]  # Reset truck usage history\n",
    "    \n",
    "        state = np.array([\n",
    "            self.inventory_level,\n",
    "            self.current_time,\n",
    "            self.backlog,\n",
    "            self.expected_demand[self.current_time],\n",
    "            self.lead_time[self.current_time],\n",
    "            next_day_demand,\n",
    "            self.recent_truck_usage[0],\n",
    "            self.recent_truck_usage[1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "env = ConstructionLogisticsEnv()\n",
    "dqn = DQN(input_dim=8, output_dim=env.action_space.n)\n",
    "target_dqn = DQN(input_dim=8, output_dim=env.action_space.n)\n",
    "target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.991 #0.995 #0.997\n",
    "epsilon_min = 0.05 \n",
    "num_episodes = 8000\n",
    "batch_size = 64\n",
    "memory = deque(maxlen=50000)\n",
    "soft_tau = 0.05\n",
    "\n",
    "ema_alpha = 0.05\n",
    "\n",
    "def ema(values, alpha):\n",
    "    ema_values = []\n",
    "    for i, v in enumerate(values):\n",
    "        if i == 0:\n",
    "            ema_values.append(v)\n",
    "        else:\n",
    "            ema_values.append(alpha * v + (1 - alpha) * ema_values[-1])\n",
    "    return ema_values\n",
    "\n",
    "rewards_per_episode = []\n",
    "losses_per_episode = []\n",
    "q_variances = []\n",
    "\n",
    "shaped_rewards_per_episode = []\n",
    "true_costs_per_episode = []\n",
    "\n",
    "def choose_action(state):\n",
    "    # Get the current inventory level\n",
    "    inventory_level = state[0]\n",
    "    \n",
    "    # Only allow actions that don't exceed max inventory\n",
    "    possible_actions = [i for i, qty in enumerate(env.valid_order_quantities) if inventory_level + qty <= env.max_inventory]\n",
    "\n",
    "    # If the inventory is full, only allow action 0 (no order)\n",
    "    if inventory_level >= env.max_inventory:\n",
    "        possible_actions = [0]\n",
    "            \n",
    "    # If epsilon-greedy, choose randomly from the valid actions\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(possible_actions)\n",
    "    \n",
    "    # Otherwise, choose the best action from the valid actions\n",
    "    with torch.no_grad():\n",
    "        q_values = dqn(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).squeeze(0)\n",
    "        valid_q_values = [q_values[action] for action in possible_actions]\n",
    "        best_action = possible_actions[torch.argmax(torch.tensor(valid_q_values)).item()]\n",
    "        return best_action\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "   \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    total_cost = 0  # New: for real-world cost\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        real_cost = -reward if \"cost\" not in _ else _[\"cost\"]\n",
    "        \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        total_cost += real_cost\n",
    "        \n",
    "    rewards_per_episode.append(total_reward)\n",
    "        \n",
    "    shaped_rewards_per_episode.append(total_reward)\n",
    "    true_costs_per_episode.append(total_cost)\n",
    "\n",
    "    if len(memory) > batch_size:\n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "\n",
    "        q_values = dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = target_dqn(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (0.99 * next_q_values * (~dones))\n",
    "        loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Soft update\n",
    "        for target_param, param in zip(target_dqn.parameters(), dqn.parameters()):\n",
    "            target_param.data.copy_(soft_tau * param.data + (1.0 - soft_tau) * target_param.data)\n",
    "\n",
    "        losses_per_episode.append(loss.item())\n",
    "        q_variances.append(dqn(states).var().item())\n",
    "   \n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "        \n",
    "# Apply EMA smoothing\n",
    "rewards_ema = ema(rewards_per_episode, ema_alpha)\n",
    "loss_ema = ema(losses_per_episode, ema_alpha)\n",
    "q_var_ema = ema(q_variances, ema_alpha)\n",
    "\n",
    "# Plot Performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rewards_per_episode, alpha=0.3, label='Raw')\n",
    "plt.plot(rewards_ema, label='EMA Smoothed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(losses_per_episode, alpha=0.3, label='Raw')\n",
    "plt.plot(loss_ema, label='EMA Smoothed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Bellman Error (Loss)')\n",
    "plt.title('Bellman Error Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(q_variances)\n",
    "plt.plot(q_var_ema, label='Q-Variance EMA')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Q-Value Variance')\n",
    "plt.title('Q-Value Variance Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(shaped_rewards_per_episode, label=\"Shaped Reward (used for training)\")\n",
    "plt.plot([-c for c in true_costs_per_episode], label=\"True Total Cost (negated)\", linestyle=\"--\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward / -Cost\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Progress: Shaped Reward vs True Cost\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "torch.save(dqn.state_dict(), \"trained_dqn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdccb3-cfc5-4125-8f98-0d746a963a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
